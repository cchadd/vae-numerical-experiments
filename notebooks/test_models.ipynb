{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-47e4a1c708cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnotebooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#from trainers.trainer import ModelTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from .notebooks import *\n",
    "#from trainers.trainer import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "bs= 128\n",
    "\n",
    "train = datasets.MNIST(root='./data/',\n",
    "                       train=True,\n",
    "                       transform=transforms.ToTensor(),\n",
    "                       download=True)\n",
    "\n",
    "test = datasets.MNIST(root='./data/',\n",
    "                    train=False,\n",
    "                    transform=transforms.ToTensor(),\n",
    "                    download=True) \n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ModelTrainer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 train_loader,\n",
    "                 test_loader,\n",
    "                 optimizer='adam',\n",
    "                 lr=1e-3,\n",
    "                 batch_size=100,\n",
    "                 n_epochs=15,\n",
    "                 seed=1,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "            The ModelTrainer module\n",
    "            \n",
    "            Inputs:\n",
    "            -------\n",
    "            model (model):\n",
    "                - The model to train. It should contain a loss_fonction method taken as\n",
    "                convergence criteria\n",
    "            train_loader (DataLoader):\n",
    "                - DataLoader containng the train dataset\n",
    "            test_loader (DataLoader):\n",
    "                - DataLoader containing the test dataset\n",
    "            optimizer (str):\n",
    "                - The optimizer's name\n",
    "            lr (float):\n",
    "                - The learning rate to use\n",
    "            batch_size (int):\n",
    "                - The batch_size for training\n",
    "            n_epochs (int):\n",
    "                - The number of epochs for training\n",
    "            seed (int):\n",
    "                - The random seed to use\n",
    "            verbose (Bool):\n",
    "                - Verbosity\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.n_epochs = n_epochs\n",
    "        self.verbose = True\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"Optimizer {optimizer} is not defined\")\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.__train_epoch(epoch)\n",
    "            self.__test_epoch()\n",
    "\n",
    "    def __train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(self.train_loader):\n",
    "            data = data.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            recon_batch, mu, log_var = self.model(data)\n",
    "            loss = self.model.loss_function(recon_batch, data, mu, log_var)\n",
    "    \n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            if batch_idx % 100 == 0 and self.verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(self.train_loader.dataset),\n",
    "                    100. * batch_idx / len(self.train_loader), loss.item() / len(data)))\n",
    "                \n",
    "        if self.verbose:\n",
    "            print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(self.train_loader.dataset)))\n",
    "    \n",
    "\n",
    "    def __test_epoch(self):\n",
    "        self.model.eval()\n",
    "        test_loss= 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in self.test_loader:\n",
    "                data = data.to(self.device)\n",
    "                recon, mu, log_var = self.model(data)\n",
    "        \n",
    "                # sum up batch loss\n",
    "                test_loss += self.model.loss_function(recon, data, mu, log_var).item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "\n",
    "        if self.verbose:\n",
    "            print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 552.344604\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 187.776321\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 182.135422\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 176.360733\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 175.167053\n",
      "====> Epoch: 0 Average loss: 189.8527\n",
      "====> Test set loss: 169.8956\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 171.591293\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 174.289261\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 162.901184\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 161.025787\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 157.064270\n",
      "====> Epoch: 1 Average loss: 167.4531\n",
      "====> Test set loss: 163.9373\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 168.132263\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 165.537109\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 167.621857\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 168.752167\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 164.822693\n",
      "====> Epoch: 2 Average loss: 163.4267\n",
      "====> Test set loss: 161.5803\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 161.159531\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 148.957077\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 159.159988\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 165.395706\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 158.127975\n",
      "====> Epoch: 3 Average loss: 161.1916\n",
      "====> Test set loss: 159.9973\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 163.185104\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 161.814667\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 157.745193\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 161.708115\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 151.922806\n",
      "====> Epoch: 4 Average loss: 159.5996\n",
      "====> Test set loss: 158.8305\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 160.104156\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 161.384583\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 160.181046\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 157.876007\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 154.712463\n",
      "====> Epoch: 5 Average loss: 158.4059\n",
      "====> Test set loss: 157.9415\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 154.784836\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 163.456421\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 157.734283\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 164.615524\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 152.809372\n",
      "====> Epoch: 6 Average loss: 157.3693\n",
      "====> Test set loss: 156.8389\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 150.196045\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 159.541809\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 158.284317\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 155.265167\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 155.186462\n",
      "====> Epoch: 7 Average loss: 156.4455\n",
      "====> Test set loss: 156.4525\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 150.672440\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer(vae, train_loader, test_loader)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 2).cuda()\n",
    "    sample = vae.decode(z).cuda()\n",
    "    sample = torch.reshape(sample, (64, 28, 28))\n",
    "plt.matshow(sample.cpu().numpy()[0])"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
